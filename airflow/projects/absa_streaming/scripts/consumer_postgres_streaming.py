# SE363 â€“ PhÃ¡t triá»ƒn á»©ng dá»¥ng trÃªn ná»n táº£ng dá»¯ liá»‡u lá»›n
# Khoa CÃ´ng nghá»‡ Pháº§n má»m â€“ TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin, ÄHQG-HCM
# HopDT â€“ Faculty of Software Engineering, University of Information Technology (FSE-UIT)
#
# ======================================
# consumer_postgres_streaming.py (PhiÃªn báº£n CNN siÃªu nháº¹)
# ÄÃƒ TÃI Cáº¤U TRÃšC (REFACTORED) THÃ€NH CÃC HÃ€M
# ======================================

import json
import os
import re
import sys
import time
import traceback  # ThÃªm thÆ° viá»‡n Ä‘á»ƒ in lá»—i chi tiáº¿t

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as tF
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql.functions import col, from_json, pandas_udf
from pyspark.sql.streaming import StreamingQuery

# =============================================================================
# === 1. Cáº¤U HÃŒNH TOÃ€N Cá»¤C (CONSTANTS) ===
# =============================================================================
ASPECTS = [
    "Price",
    "Shipping",
    "Outlook",
    "Quality",
    "Size",
    "Shop_Service",
    "General",
    "Others",
]
SENTIMENTS = ["POS", "NEU", "NEG"]

# ÄÆ°á»ng dáº«n (pháº£i khá»›p vá»›i volumes trong DockerOperator)
CHECKPOINT_PATH = "/opt/airflow/checkpoints/absa_streaming_checkpoint"
VOCAB_PATH = "/opt/airflow/models/vocab.json"
MODEL_PATH = "/opt/airflow/models/cnn_best.pth"

# Cáº¥u hÃ¬nh model
MAX_LEN = 64
EMBED_DIM = 100
DEVICE = "cpu"

# Biáº¿n global cho model (dÃ¹ng trong UDF)
_model, _vocab = None, None


# =============================================================================
# === 2. Äá»ŠNH NGHÄ¨A MODEL VÃ€ HÃ€M Há»– TRá»¢ ===
# (Pháº£i á»Ÿ top-level Ä‘á»ƒ Spark cÃ³ thá»ƒ "tháº¥y" vÃ  "serialize")
# =============================================================================


class CNN_ABSAModel(nn.Module):
    """Äá»‹nh nghÄ©a kiáº¿n trÃºc mÃ´ hÃ¬nh CNN cho ABSA."""

    def __init__(
        self,
        vocab_size,
        embed_dim,
        num_aspects,
        num_sentiments=3,
        num_filters=50,
        kernel_sizes=[3, 4, 5],
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.convs = nn.ModuleList(
            [
                nn.Conv1d(
                    in_channels=embed_dim, out_channels=num_filters, kernel_size=k
                )
                for k in kernel_sizes
            ]
        )
        total_filters = num_filters * len(kernel_sizes)
        self.dropout = nn.Dropout(0.1)
        self.head_s = nn.Linear(total_filters, num_aspects * num_sentiments)

    def forward(self, input_ids):
        embedded = self.embedding(input_ids).permute(0, 2, 1)
        conved = [F.relu(conv(embedded)) for conv in self.convs]
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        cat = self.dropout(torch.cat(pooled, dim=1))
        logits_s = self.head_s(cat).view(-1, len(ASPECTS), 3)
        return logits_s


def text_to_indices(text: str, vocab: dict, max_len: int) -> list:
    """Helper: Chuyá»ƒn text thÃ´ sang list cÃ¡c ID tá»« vá»±ng."""
    tokens = re.findall(r"\w+", text.lower())
    indices = [vocab.get(token, vocab.get("<unk>", 1)) for token in tokens]
    if len(indices) < max_len:
        indices += [vocab.get("<pad>", 0)] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    return indices


@pandas_udf(T.MapType(T.StringType(), T.StringType()))
def absa_cnn_infer_and_decode_udf(texts: pd.Series) -> pd.Series:
    """
    Pandas UDF: Táº£i model (má»™t láº§n) vÃ  thá»±c hiá»‡n dá»± Ä‘oÃ¡n (inference)
    cho tá»«ng batch dá»¯ liá»‡u.
    """
    global _model, _vocab
    if _model is None:
        try:
            # 1. Táº£i vocab
            print("UDF: Äang táº£i vocab...")
            with open(VOCAB_PATH, "r", encoding="utf-8") as f:
                _vocab = json.load(f)
            print(f"UDF: Táº£i vocab thÃ nh cÃ´ng ({len(_vocab)} tá»«).")

            # 2. Táº£i model CNN
            print("UDF: Äang táº£i model CNN...")
            _model = CNN_ABSAModel(
                vocab_size=len(_vocab), embed_dim=EMBED_DIM, num_aspects=len(ASPECTS)
            )
            _model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
            _model.to(DEVICE).eval()
            print("UDF: Táº£i model CNN thÃ nh cÃ´ng.")

        except Exception as e:
            print(
                f"Lá»–I NGHIÃŠM TRá»ŒNG TRONG UDF: KhÃ´ng thá»ƒ táº£i model/vocab. Lá»—i: {e}",
                file=sys.stderr,
            )
            return pd.Series([{} for _ in range(len(texts))])

    # 3. Tokenize toÃ n bá»™ batch
    all_indices = [text_to_indices(text, _vocab, MAX_LEN) for text in texts]
    input_tensor = torch.tensor(all_indices, dtype=torch.long).to(DEVICE)

    # 4. Infer toÃ n bá»™ batch
    with torch.no_grad():
        logits_s = _model(input_tensor)
        sent_indices = torch.argmax(logits_s, dim=-1).cpu().numpy()

    # 5. Decode káº¿t quáº£
    results = []
    sent_map = np.array(SENTIMENTS)
    for batch_indices in sent_indices:
        sent_labels = sent_map[batch_indices]
        res_map = {asp: sent for asp, sent in zip(ASPECTS, sent_labels)}
        results.append(res_map)

    return pd.Series(results)


# =============================================================================
# === 3. Äá»ŠNH NGHÄ¨A HÃ€M GHI (SINK) ===
# (HÃ m nÃ y Ä‘Æ°á»£c gá»i bá»Ÿi foreachBatch)
# =============================================================================


def write_to_postgres(batch_df: DataFrame, batch_id: int):
    """
    Ghi má»™t micro-batch DataFrame vÃ o báº£ng PostgreSQL.
    """
    sys.stdout.reconfigure(encoding="utf-8")

    # Tá»‘i Æ°u: cache batch nÃ y Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i
    batch_df.persist()
    total_rows = 0

    try:
        total_rows = batch_df.count()
        if total_rows == 0:
            print(f"[Batch {batch_id}] âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i.")
            return

        # Log preview
        preview_pd = batch_df.select("review", *ASPECTS).limit(5).toPandas()
        preview_dict = preview_pd.to_dict(orient="records")
        print(
            f"\n[Batch {batch_id}] Nháº­n {total_rows} dÃ²ng, hiá»ƒn thá»‹ {len(preview_dict)} dÃ²ng Ä‘áº§u (CNN):"
        )
        print(json.dumps(preview_dict, ensure_ascii=False, indent=2))

        # Ghi vÃ o DB
        (
            batch_df.select(F.col("review").alias("ReviewText"), *ASPECTS)
            .write.format("jdbc")
            .option("url", "jdbc:postgresql://postgres:5432/airflow")
            .option("dbtable", "absa_results")
            .option("user", "airflow")
            .option("password", "airflow")
            .option("driver", "org.postgresql.Driver")
            .option("charset", "utf8")
            .mode("append")
            .save()
        )
        print(f"[Batch {batch_id}] âœ… Ghi PostgreSQL thÃ nh cÃ´ng ({total_rows} dÃ²ng).")

    except Exception as e:
        print(
            f"[Batch {batch_id}] âš ï¸ KhÃ´ng thá»ƒ ghi vÃ o PostgreSQL. Lá»—i: {str(e)}",
            file=sys.stderr,
        )
        traceback.print_exc()  # In chi tiáº¿t lá»—i
    finally:
        # Giáº£i phÃ³ng cache
        batch_df.unpersist()


# =============================================================================
# === 4. CÃC HÃ€M Xá»¬ LÃ CHÃNH (PIPELINE) ===
# =============================================================================


def create_spark_session() -> SparkSession:
    """Khá»Ÿi táº¡o vÃ  tráº£ vá» má»™t SparkSession."""
    print("Pipeline: Äang táº¡o Spark session...")
    spark = (
        SparkSession.builder.appName("Kafka_ABSA_Postgres_CNN")
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH)
        .config(
            "spark.jars.packages",
            ",".join(
                [
                    "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2",
                    "org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.2",
                    "org.apache.kafka:kafka-clients:3.5.1",
                    "org.apache.commons:commons-pool2:2.12.0",
                    "org.postgresql:postgresql:42.6.0",
                ]
            ),
        )
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    print("Pipeline: Táº¡o Spark session thÃ nh cÃ´ng.")
    return spark


def define_kafka_source(spark: SparkSession) -> DataFrame:
    """Äá»‹nh nghÄ©a nguá»“n streaming tá»« Kafka vÃ  trÃ­ch xuáº¥t text."""
    print("Pipeline: Äang Ä‘á»‹nh nghÄ©a nguá»“n Kafka...")
    df_stream = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", "kafka:9092")
        .option("subscribe", "absa-reviews")
        .option("startingOffsets", "latest")
        .option("maxOffsetsPerTrigger", 100)
        .load()
    )

    review_schema = T.StructType(
        [T.StructField("id", T.StringType()), T.StructField("review", T.StringType())]
    )
    df_json = df_stream.select(
        from_json(col("value").cast("string"), review_schema).alias("data")
    )
    df_text = df_json.select(F.col("data.review").alias("review"))
    print("Pipeline: Äá»‹nh nghÄ©a nguá»“n Kafka thÃ nh cÃ´ng.")
    return df_text


def process_stream(df_in: DataFrame) -> DataFrame:
    """Ãp dá»¥ng UDF vÃ  chuyá»ƒn Ä‘á»•i DataFrame."""
    print("Pipeline: Äang Ã¡p dá»¥ng logic xá»­ lÃ½ (UDF)...")
    df_pred = df_in.withColumn(
        "aspect_sentiments", absa_cnn_infer_and_decode_udf(F.col("review"))
    )

    df_final = df_pred.select("review", "aspect_sentiments")
    for asp in ASPECTS:
        df_final = df_final.withColumn(asp, F.col("aspect_sentiments").getItem(asp))

    print("Pipeline: Ãp dá»¥ng logic xá»­ lÃ½ thÃ nh cÃ´ng.")
    return df_final


def start_stream_sink(df_final: DataFrame) -> StreamingQuery:
    """Báº¯t Ä‘áº§u query streaming vÃ  ghi ra sink (Postgres)."""
    print("Pipeline: Äang báº¯t Ä‘áº§u streaming query...")
    query = (
        df_final.writeStream.foreachBatch(write_to_postgres)
        .outputMode("append")
        .trigger(processingTime="5 seconds")
        .start()
    )
    print(
        "ğŸš€ Streaming job (CNN SiÃªu Nháº¹) started â€” Ä‘ang láº¯ng nghe dá»¯ liá»‡u tá»« Kafka..."
    )
    return query


# =============================================================================
# === 5. THá»°C THI CHÆ¯Æ NG TRÃŒNH ===
# =============================================================================


def main():
    """HÃ m main Ä‘iá»u phá»‘i toÃ n bá»™ pipeline."""
    try:
        spark = create_spark_session()
        df_raw = define_kafka_source(spark)
        df_final = process_stream(df_raw)
        query = start_stream_sink(df_final)
        query.awaitTermination()
    except Exception as e:
        print(f"FATAL ERROR: Pipeline Ä‘Ã£ bá»‹ dá»«ng Ä‘á»™t ngá»™t. Lá»—i: {e}", file=sys.stderr)
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
