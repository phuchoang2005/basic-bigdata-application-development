# SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# consumer_postgres_streaming.py
# ======================================
# Consumer ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka topic "absa-reviews"
# ‚Üí ch·∫°y inference m√¥ h√¨nh ABSA (.pt)
# ‚Üí ghi k·∫øt qu·∫£ v√†o PostgreSQL
# ‚Üí Airflow s·∫Ω gi√°m s√°t v√† kh·ªüi ƒë·ªông l·∫°i khi job b·ªã d·ª´ng.
# SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# consumer_postgres_streaming.py (ƒê√É T·ªêI ∆ØU)
# ======================================
# Consumer ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka topic "absa-reviews"
# ‚Üí ch·∫°y inference m√¥ h√¨nh ABSA (vectorized)
# ‚Üí ghi k·∫øt qu·∫£ v√†o PostgreSQL (s·ª≠ d·ª•ng cache)
# consumer_postgres_streaming.py (Phi√™n b·∫£n CNN si√™u nh·∫π)
# ======================================
# B·ªè ho√†n to√†n 'transformers', ch·ªâ d√πng 'torch' v√† 'pyspark'.
# Nhanh h∆°n, nh·∫π h∆°n, ph√π h·ª£p cho m√°y y·∫øu.

import json
import os
import re
import sys
import time

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as tF
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql.functions import col, from_json, pandas_udf

# === 1. Spark session (Gi·ªØ nguy√™n) ===
spark = (
    SparkSession.builder.appName("Kafka_ABSA_Postgres_CNN")
    .config(
        "spark.sql.streaming.checkpointLocation",
        "/opt/airflow/checkpoints/absa_streaming_checkpoint",
    )
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# === 2. ƒê·ªçc d·ªØ li·ªáu streaming t·ª´ Kafka (Gi·ªØ nguy√™n) ===
df_stream = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", "kafka:9092")
    .option("subscribe", "absa-reviews")
    .option("startingOffsets", "latest")
    .option("maxOffsetsPerTrigger", 100)  # X·ª≠ l√Ω 100 d√≤ng m·ªói 5s
    .load()
)

review_schema = T.StructType(
    [T.StructField("id", T.StringType()), T.StructField("review", T.StringType())]
)
df_json = df_stream.select(
    from_json(col("value").cast("string"), review_schema).alias("data")
)
df_text = df_json.select(F.col("data.review").alias("review"))  # ƒê·∫∑t alias l√† "review"

# === 3. ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN (THAY TH·∫æ HO√ÄN TO√ÄN) ===
ASPECTS = [
    "Price",
    "Shipping",
    "Outlook",
    "Quality",
    "Size",
    "Shop_Service",
    "General",
    "Others",
]
SENTIMENTS = ["POS", "NEU", "NEG"]

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn model CNN v√† vocab c·ªßa b·∫°n
VOCAB_PATH = "/opt/models/vocab.json"
MODEL_PATH = "/opt/models/cnn_best.pth"

MAX_LEN = 64  # Max length khi hu·∫•n luy·ªán CNN
EMBED_DIM = 100  # V√≠ d·ª•
DEVICE = "cpu"  # √âp ch·∫°y CPU cho nh·∫π

_model, _vocab = None, None


# ƒê·ªãnh nghƒ©a m·ªôt m√¥ h√¨nh CNN (Ki·ªÉu "Kim CNN" 2014)
class CNN_ABSAModel(nn.Module):
    def __init__(
        self,
        vocab_size,
        embed_dim,
        num_aspects,
        num_sentiments=3,
        num_filters=50,
        kernel_sizes=[3, 4, 5],
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        self.convs = nn.ModuleList(
            [
                nn.Conv1d(
                    in_channels=embed_dim, out_channels=num_filters, kernel_size=k
                )
                for k in kernel_sizes
            ]
        )

        total_filters = num_filters * len(kernel_sizes)
        self.dropout = nn.Dropout(0.1)
        self.head_s = nn.Linear(total_filters, num_aspects * num_sentiments)

    def forward(self, input_ids):
        # input_ids shape: [batch_size, max_len]
        embedded = self.embedding(input_ids)
        # embedded shape: [batch_size, max_len, embed_dim]

        embedded = embedded.permute(0, 2, 1)
        # embedded shape: [batch_size, embed_dim, max_len] (cho Conv1d)

        conved = [F.relu(conv(embedded)) for conv in self.convs]
        # conved[i] shape: [batch_size, num_filters, max_len - k + 1]

        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        # pooled[i] shape: [batch_size, num_filters] (Global Max Pooling)

        cat = self.dropout(torch.cat(pooled, dim=1))
        # cat shape: [batch_size, num_filters * len(kernel_sizes)]

        logits_s = self.head_s(cat).view(-1, len(ASPECTS), 3)
        return logits_s


# Helper: H√†m tokenizer ƒë∆°n gi·∫£n cho CNN (thay th·∫ø AutoTokenizer)
def text_to_indices(text, vocab, max_len):
    tokens = re.findall(r"\w+", text.lower())  # T√°ch t·ª´ ƒë∆°n gi·∫£n
    indices = [vocab.get(token, vocab.get("<unk>", 1)) for token in tokens]
    # Padding
    if len(indices) < max_len:
        indices += [vocab.get("<pad>", 0)] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    return indices


# UDF m·ªõi: D√πng CNN, ƒë√£ vector h√≥a v√† g·ªôp logic
@pandas_udf(T.MapType(T.StringType(), T.StringType()))
def absa_cnn_infer_and_decode_udf(texts: pd.Series) -> pd.Series:
    global _model, _vocab
    if _model is None:
        # 1. T·∫£i vocab (file JSON)
        try:
            with open(VOCAB_PATH, "r", encoding="utf-8") as f:
                _vocab = json.load(f)
        except Exception as e:
            # Ghi l·ªói nghi√™m tr·ªçng n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c vocab
            print(f"L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫£i vocab t·ª´ {VOCAB_PATH}. L·ªói: {e}")
            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ r·ªóng cho batch n√†y
            return pd.Series([{} for _ in range(len(texts))])

        # 2. T·∫£i model CNN
        # Ph·∫£i kh·ªõp v·ªõi tham s·ªë khi hu·∫•n luy·ªán
        _model = CNN_ABSAModel(
            vocab_size=len(_vocab), embed_dim=EMBED_DIM, num_aspects=len(ASPECTS)
        )
        _model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
        _model.to(DEVICE).eval()

    # 3. Tokenize to√†n b·ªô batch (vectorized)
    # (Vi·ªác tokenize kh√¥ng th·ªÉ vector h√≥a ho√†n to√†n nh∆∞ transformers,
    # nh∆∞ng list comprehension v·∫´n nhanh)
    all_indices = [text_to_indices(text, _vocab, MAX_LEN) for text in texts]
    input_tensor = torch.tensor(all_indices, dtype=torch.long).to(DEVICE)

    # 4. Infer to√†n b·ªô batch
    with torch.no_grad():
        logits_s = _model(input_tensor)
        sent_indices = torch.argmax(logits_s, dim=-1).cpu().numpy()

    # 5. Decode k·∫øt qu·∫£
    results = []
    sent_map = np.array(SENTIMENTS)
    for batch_indices in sent_indices:
        sent_labels = sent_map[batch_indices]
        res_map = {asp: sent for asp, sent in zip(ASPECTS, sent_labels)}
        results.append(res_map)

    return pd.Series(results)


# === 4. √Åp d·ª•ng UDF v√† t·∫°o c·ªôt (Gi·ªØ nguy√™n) ===
df_pred = df_text.withColumn(
    "aspect_sentiments", absa_cnn_infer_and_decode_udf(F.col("review"))
)

df_final = df_pred.select("review", "aspect_sentiments")
for asp in ASPECTS:
    df_final = df_final.withColumn(asp, F.col("aspect_sentiments").getItem(asp))


# === 5. Ghi k·∫øt qu·∫£ v√†o PostgreSQL (Gi·ªØ nguy√™n) ===
def write_to_postgres(batch_df, batch_id):
    sys.stdout.reconfigure(encoding="utf-8")
    batch_df.persist()  # Gi·ªØ nguy√™n t·ªëi ∆∞u cache

    total_rows = 0
    try:
        total_rows = batch_df.count()
        if total_rows == 0:
            print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
            return

        # Log preview
        preview = (
            batch_df.select("review", *ASPECTS)
            .limit(5)
            .toPandas()
            .to_dict(orient="records")
        )
        print(
            f"\n[Batch {batch_id}] Nh·∫≠n {total_rows} d√≤ng, hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu (CNN):"
        )
        print(json.dumps(preview, ensure_ascii=False, indent=2))

        # Ghi v√†o DB
        (
            batch_df.select(
                F.col("review").alias("ReviewText"), *ASPECTS
            )  # ƒê·ªïi t√™n c·ªôt
            .write.format("jdbc")
            .option("url", "jdbc:postgresql://postgres:5432/airflow")
            .option("dbtable", "absa_results")
            .option("user", "airflow")
            .option("password", "airflow")
            .option("driver", "org.postgresql.Driver")
            .option("charset", "utf8")
            .mode("append")
            .save()
        )
        print(f"[Batch {batch_id}] ‚úÖ Ghi PostgreSQL th√†nh c√¥ng ({total_rows} d√≤ng).")

    except Exception as e:
        print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng th·ªÉ ghi v√†o PostgreSQL. L·ªói: {str(e)}")
    finally:
        batch_df.unpersist()  # Gi·ªØ nguy√™n t·ªëi ∆∞u cache


# === 6. B·∫Øt ƒë·∫ßu stream (Gi·ªØ nguy√™n) ===
query = (
    df_final.writeStream.foreachBatch(write_to_postgres)
    .outputMode("append")
    .trigger(processingTime="5 seconds")
    .start()
)

print("üöÄ Streaming job (CNN Si√™u Nh·∫π) started ‚Äî ƒëang l·∫Øng nghe d·ªØ li·ªáu t·ª´ Kafka...")
query.awaitTermination()
