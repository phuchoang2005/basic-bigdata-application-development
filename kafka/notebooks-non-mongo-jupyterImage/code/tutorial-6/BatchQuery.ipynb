{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585c3194-a2e4-462d-828d-747d5f343d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d735147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://1bd6cb8e60c5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f00340cd210>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_version='3.5.3'\n",
    "scala_version='2.12'\n",
    "spark_connector_version='3.5.3'\n",
    "kafka_client_version='3.5.1'\n",
    "common_pool2_version='2.12.0'\n",
    "\n",
    "packages = [\n",
    "    f\"spark-sql-kafka-0-10_{scala_version}-{spark_connector_version}.jar\",\n",
    "    f\"kafka-clients-{kafka_client_version}.jar\",\n",
    "    f\"commons-pool2-{common_pool2_version}.jar\",\n",
    "    f\"spark-token-provider-kafka-0-10_{scala_version}-{spark_version}.jar\"\n",
    "]\n",
    "\n",
    "SPARK_HOME = \"/usr/local/spark\"\n",
    "SPARK_JARS_DIR = SPARK_HOME + \"/jars/\"\n",
    "\n",
    "packages = [SPARK_JARS_DIR + p for p in packages]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"kafka-example\") \\\n",
    "    .config(\"spark.jars\", \",\".join(packages)) \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6c157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:16:20 INFO CodeGenerator: Code generated in 430.047637 ms\n",
      "25/11/25 02:16:20 INFO CodeGenerator: Code generated in 54.252479 ms\n"
     ]
    }
   ],
   "source": [
    "kafka_server = \"kafka:9092\" \n",
    "topic_name = \"LeNguyenHoangPhuc_RandomNumber\"         \n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd8e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:20:33 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_150/4060790680.py:1\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Got job 14 (toPandas at /tmp/ipykernel_150/4060790680.py:1) with 1 output partitions\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Final stage: ResultStage 19 (toPandas at /tmp/ipykernel_150/4060790680.py:1)\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[58] at toPandas at /tmp/ipykernel_150/4060790680.py:1), which has no missing parents\n",
      "25/11/25 02:20:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 39.8 KiB, free 434.3 MiB)\n",
      "25/11/25 02:20:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.3 MiB)\n",
      "25/11/25 02:20:33 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1bd6cb8e60c5:34235 (size: 16.3 KiB, free: 434.4 MiB)\n",
      "25/11/25 02:20:33 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[58] at toPandas at /tmp/ipykernel_150/4060790680.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/11/25 02:20:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/11/25 02:20:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14) (1bd6cb8e60c5, executor driver, partition 0, PROCESS_LOCAL, 9203 bytes) \n",
      "25/11/25 02:20:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 14)\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Seeking to earliest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Seeking to latest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=386, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:20:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Seeking to offset 0 for partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Seeking to earliest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Seeking to latest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=387, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:20:33 INFO KafkaDataConsumer: From Kafka topicPartition=LeNguyenHoangPhuc_RandomNumber-0 groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor read 386 records through 1 polls (polled  out 386 records), taking 151873249 nanos, during time span of 223551551 nanos.\n",
      "25/11/25 02:20:33 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1bd6cb8e60c5:34235 in memory (size: 24.3 KiB, free: 434.4 MiB)\n",
      "25/11/25 02:20:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 14). 8371 bytes result sent to driver\n",
      "25/11/25 02:20:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 249 ms on 1bd6cb8e60c5 (executor driver) (1/1)\n",
      "25/11/25 02:20:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/11/25 02:20:33 INFO DAGScheduler: ResultStage 19 (toPandas at /tmp/ipykernel_150/4060790680.py:1) finished in 0.263 s\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/11/25 02:20:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/11/25 02:20:33 INFO DAGScheduler: Job 14 finished: toPandas at /tmp/ipykernel_150/4060790680.py:1, took 0.273718 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-11-25 02:16:55.071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-25 02:16:57.073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-11-25 02:16:59.075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-25 02:17:01.080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-11-25 02:17:10.964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td>2025-11-25 02:20:31.288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "      <td>2025-11-25 02:20:31.791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>383</td>\n",
       "      <td>2025-11-25 02:20:32.295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>2025-11-25 02:20:32.799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "      <td>2025-11-25 02:20:33.300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      key                                              value  \\\n",
       "0    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "1    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "2    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "3    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "4    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "..    ...                                                ...   \n",
       "381  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "382  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "383  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "384  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "385  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...   \n",
       "\n",
       "                              topic  partition  offset  \\\n",
       "0    LeNguyenHoangPhuc_RandomNumber          0       0   \n",
       "1    LeNguyenHoangPhuc_RandomNumber          0       1   \n",
       "2    LeNguyenHoangPhuc_RandomNumber          0       2   \n",
       "3    LeNguyenHoangPhuc_RandomNumber          0       3   \n",
       "4    LeNguyenHoangPhuc_RandomNumber          0       4   \n",
       "..                              ...        ...     ...   \n",
       "381  LeNguyenHoangPhuc_RandomNumber          0     381   \n",
       "382  LeNguyenHoangPhuc_RandomNumber          0     382   \n",
       "383  LeNguyenHoangPhuc_RandomNumber          0     383   \n",
       "384  LeNguyenHoangPhuc_RandomNumber          0     384   \n",
       "385  LeNguyenHoangPhuc_RandomNumber          0     385   \n",
       "\n",
       "                  timestamp  timestampType  \n",
       "0   2025-11-25 02:16:55.071              0  \n",
       "1   2025-11-25 02:16:57.073              0  \n",
       "2   2025-11-25 02:16:59.075              0  \n",
       "3   2025-11-25 02:17:01.080              0  \n",
       "4   2025-11-25 02:17:10.964              0  \n",
       "..                      ...            ...  \n",
       "381 2025-11-25 02:20:31.288              0  \n",
       "382 2025-11-25 02:20:31.791              0  \n",
       "383 2025-11-25 02:20:32.295              0  \n",
       "384 2025-11-25 02:20:32.799              0  \n",
       "385 2025-11-25 02:20:33.300              0  \n",
       "\n",
       "[386 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701287cb-4a1a-4473-a825-5e4c34c8e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:22:12 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_150/4067002047.py:11\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Got job 36 (toPandas at /tmp/ipykernel_150/4067002047.py:11) with 1 output partitions\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Final stage: ResultStage 41 (toPandas at /tmp/ipykernel_150/4067002047.py:11)\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[123] at toPandas at /tmp/ipykernel_150/4067002047.py:11), which has no missing parents\n",
      "25/11/25 02:22:12 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 39.4 KiB, free 434.3 MiB)\n",
      "25/11/25 02:22:12 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.2 MiB)\n",
      "25/11/25 02:22:12 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 1bd6cb8e60c5:34235 (size: 16.3 KiB, free: 434.4 MiB)\n",
      "25/11/25 02:22:12 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[123] at toPandas at /tmp/ipykernel_150/4067002047.py:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/11/25 02:22:12 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "25/11/25 02:22:12 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 36) (1bd6cb8e60c5, executor driver, partition 0, PROCESS_LOCAL, 9203 bytes) \n",
      "25/11/25 02:22:12 INFO Executor: Running task 0.0 in stage 41.0 (TID 36)\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to earliest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to latest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 2\n",
      "This code is set up by Le Nguyen Hoang Phuc - 23521198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=582, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to offset 0 for partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to earliest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to latest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=582, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to offset 500 for partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to earliest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Seeking to latest offset of partition LeNguyenHoangPhuc_RandomNumber-0\n",
      "25/11/25 02:22:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting offset for partition LeNguyenHoangPhuc_RandomNumber-0 to position FetchPosition{offset=583, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "25/11/25 02:22:12 INFO KafkaDataConsumer: From Kafka topicPartition=LeNguyenHoangPhuc_RandomNumber-0 groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor read 582 records through 2 polls (polled  out 582 records), taking 317607158 nanos, during time span of 352795650 nanos.\n",
      "25/11/25 02:22:12 INFO Executor: Finished task 0.0 in stage 41.0 (TID 36). 5708 bytes result sent to driver\n",
      "25/11/25 02:22:12 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 36) in 365 ms on 1bd6cb8e60c5 (executor driver) (1/1)\n",
      "25/11/25 02:22:12 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "25/11/25 02:22:12 INFO DAGScheduler: ResultStage 41 (toPandas at /tmp/ipykernel_150/4067002047.py:11) finished in 0.376 s\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/11/25 02:22:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished\n",
      "25/11/25 02:22:12 INFO DAGScheduler: Job 36 finished: toPandas at /tmp/ipykernel_150/4067002047.py:11, took 0.386228 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>offset</th>\n",
       "      <th>rand_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>577</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>578</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>579</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>580</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>LeNguyenHoangPhuc_RandomNumber</td>\n",
       "      <td>581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              topic  offset rand_number\n",
       "0    LeNguyenHoangPhuc_RandomNumber       0           0\n",
       "1    LeNguyenHoangPhuc_RandomNumber       1           1\n",
       "2    LeNguyenHoangPhuc_RandomNumber       2           2\n",
       "3    LeNguyenHoangPhuc_RandomNumber       3           3\n",
       "4    LeNguyenHoangPhuc_RandomNumber       4           0\n",
       "..                              ...     ...         ...\n",
       "577  LeNguyenHoangPhuc_RandomNumber     577           2\n",
       "578  LeNguyenHoangPhuc_RandomNumber     578           2\n",
       "579  LeNguyenHoangPhuc_RandomNumber     579           2\n",
       "580  LeNguyenHoangPhuc_RandomNumber     580           2\n",
       "581  LeNguyenHoangPhuc_RandomNumber     581           2\n",
       "\n",
       "[582 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "Live view ended...\n"
     ]
    }
   ],
   "source": [
    "batchDF = kafkaDf.select(col('topic'),col('offset'),col('value').cast('string').substr(12,1).alias('rand_number'))\n",
    "\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "x = 0\n",
    "try:\n",
    "    while(True):\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {x*2}\")\n",
    "        print(\"This code is set up by Le Nguyen Hoang Phuc - 23521198\")\n",
    "        display(batchDF.toPandas())\n",
    "        sleep(2)\n",
    "        x += 1\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"break\")\n",
    "print(\"Live view ended...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc4cc470-0a08-44fe-b853-b396ef33a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "This code is set up by Le Nguyen Hoang Phuc - 23521198\n",
      "Seconds passed: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:23:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/11/25 02:23:23 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_150/1074021444.py:8\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Got job 58 (toPandas at /tmp/ipykernel_150/1074021444.py:8) with 1 output partitions\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Final stage: ResultStage 84 (toPandas at /tmp/ipykernel_150/1074021444.py:8)\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 83)\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[175] at toPandas at /tmp/ipykernel_150/1074021444.py:8), which has no missing parents\n",
      "25/11/25 02:23:23 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 58.5 KiB, free 434.0 MiB)\n",
      "25/11/25 02:23:23 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 434.0 MiB)\n",
      "25/11/25 02:23:23 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 1bd6cb8e60c5:34235 (size: 24.3 KiB, free: 434.3 MiB)\n",
      "25/11/25 02:23:23 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[175] at toPandas at /tmp/ipykernel_150/1074021444.py:8) (first 15 tasks are for partitions Vector(0))\n",
      "25/11/25 02:23:23 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks resource profile 0\n",
      "25/11/25 02:23:23 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 58) (1bd6cb8e60c5, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/11/25 02:23:23 INFO Executor: Running task 0.0 in stage 84.0 (TID 58)\n",
      "25/11/25 02:23:23 INFO ShuffleBlockFetcherIterator: Getting 1 (720.0 B) non-empty blocks including 1 (720.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/11/25 02:23:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/11/25 02:23:23 INFO Executor: Finished task 0.0 in stage 84.0 (TID 58). 5124 bytes result sent to driver\n",
      "25/11/25 02:23:23 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 58) in 19 ms on 1bd6cb8e60c5 (executor driver) (1/1)\n",
      "25/11/25 02:23:23 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool \n",
      "25/11/25 02:23:23 INFO DAGScheduler: ResultStage 84 (toPandas at /tmp/ipykernel_150/1074021444.py:8) finished in 0.028 s\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/11/25 02:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished\n",
      "25/11/25 02:23:23 INFO DAGScheduler: Job 58 finished: toPandas at /tmp/ipykernel_150/1074021444.py:8, took 0.035413 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rand_number  count\n",
       "0           7     22\n",
       "1           3     52\n",
       "2           8     22\n",
       "3           0      3\n",
       "4           5     22\n",
       "5           6     22\n",
       "6           9     22\n",
       "7           1    223\n",
       "8           4     22\n",
       "9           2    223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:23:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-8e6d37ed-6836-4a40-9693-3dbd9a990410-executor-1, groupId=spark-kafka-relation-8e6d37ed-6836-4a40-9693-3dbd9a990410-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:23:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-8e6d37ed-6836-4a40-9693-3dbd9a990410-executor-1, groupId=spark-kafka-relation-8e6d37ed-6836-4a40-9693-3dbd9a990410-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:23:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:23:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:23:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:23:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-8e6d37ed-6836-4a40-9693-3dbd9a990410-executor-1 unregistered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "Live view ended...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 02:24:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-1b3d1938-5777-46b7-9b35-e3a34fa76f9f-executor-3, groupId=spark-kafka-relation-1b3d1938-5777-46b7-9b35-e3a34fa76f9f-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:24:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-1b3d1938-5777-46b7-9b35-e3a34fa76f9f-executor-3, groupId=spark-kafka-relation-1b3d1938-5777-46b7-9b35-e3a34fa76f9f-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:24:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:24:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:24:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:24:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-1b3d1938-5777-46b7-9b35-e3a34fa76f9f-executor-3 unregistered\n",
      "25/11/25 02:24:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-e907772a-34d0-44d4-8f1f-39cecd9ae07f-executor-4, groupId=spark-kafka-relation-e907772a-34d0-44d4-8f1f-39cecd9ae07f-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:24:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-e907772a-34d0-44d4-8f1f-39cecd9ae07f-executor-4, groupId=spark-kafka-relation-e907772a-34d0-44d4-8f1f-39cecd9ae07f-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:24:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:24:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:24:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:24:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-e907772a-34d0-44d4-8f1f-39cecd9ae07f-executor-4 unregistered\n",
      "25/11/25 02:26:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:26:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2, groupId=spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:26:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:26:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:26:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:26:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-c2aa23a8-d56a-4dc4-9c66-d30edc1d0b78-executor-2 unregistered\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7, groupId=spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:27:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:27:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-f3ac198b-f2e1-44a5-bcf2-4ae53bbd3463-executor-7 unregistered\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-d79af287-50ca-477a-849e-923de9c6ff27-executor-5, groupId=spark-kafka-relation-d79af287-50ca-477a-849e-923de9c6ff27-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-d79af287-50ca-477a-849e-923de9c6ff27-executor-5, groupId=spark-kafka-relation-d79af287-50ca-477a-849e-923de9c6ff27-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:27:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:27:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-d79af287-50ca-477a-849e-923de9c6ff27-executor-5 unregistered\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-efd66c87-a947-4da8-9cb7-66d5594d97fa-executor-6, groupId=spark-kafka-relation-efd66c87-a947-4da8-9cb7-66d5594d97fa-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-efd66c87-a947-4da8-9cb7-66d5594d97fa-executor-6, groupId=spark-kafka-relation-efd66c87-a947-4da8-9cb7-66d5594d97fa-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics scheduler closed\n",
      "25/11/25 02:27:24 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/11/25 02:27:24 INFO Metrics: Metrics reporters closed\n",
      "25/11/25 02:27:24 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-efd66c87-a947-4da8-9cb7-66d5594d97fa-executor-6 unregistered\n"
     ]
    }
   ],
   "source": [
    "batchCountDF = batchDF.groupBy('rand_number').count()\n",
    "\n",
    "for x in range(0,2000):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(\"This code is set up by Le Nguyen Hoang Phuc - 23521198\")\n",
    "        print(f\"Seconds passed: {x*2}\")\n",
    "        display(batchCountDF.toPandas())\n",
    "        sleep(2)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break\n",
    "print(\"Live view ended...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c6c39-64c0-4735-b63a-4bd5b9ca116c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
