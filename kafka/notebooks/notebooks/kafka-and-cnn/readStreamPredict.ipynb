{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0185a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://51a58719cc22:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CNN and Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2d6357f050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'  # Đảm bảo đúng phiên bản Spark của bạn\n",
    "\n",
    "packages = f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},org.apache.kafka:kafka-clients:3.5.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CNN and Kafka\") \\\n",
    "    .config(\"spark.jars.packages\", packages) \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "# Các hằng số từ notebook huấn luyện của bạn\n",
    "MODEL_PATH = 'cnn_multi_aspect_model.h5'\n",
    "TOKENIZER_PATH = 'tokenizer.pickle'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "ASPECT_COLUMNS = ['Price', 'Shipping', 'Outlook', 'Quality', 'Size', 'Shop_Service', 'General', 'Others']\n",
    "\n",
    "# Ánh xạ ngược từ index (0-3) về nhãn gốc (-1, 0, 1, 2)\n",
    "label_map = {-1: 0, 0: 1, 1: 2, 2: 3}\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Broadcast đường dẫn file để các executor có thể thấy\n",
    "sc = spark.sparkContext\n",
    "broadcasted_model_path = sc.broadcast(MODEL_PATH)\n",
    "broadcasted_tokenizer_path = sc.broadcast(TOKENIZER_PATH)\n",
    "\n",
    "# Schema cho đầu ra của UDF: một mảng chứa 8 mảng con (mỗi mảng con 4 xác suất)\n",
    "schema_output = ArrayType(ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pandas import Series\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "@pandas_udf(schema_output)\n",
    "def predict_sentiments_udf(iterator: Iterator[Series]) -> Iterator[Series]:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from pickle import load\n",
    "    from re import sub\n",
    "    from os import path\n",
    "\n",
    "    model_path = broadcasted_model_path.value\n",
    "    tokenizer_path = broadcasted_tokenizer_path.value\n",
    "    \n",
    "    if not path.exists(model_path) or not path.exists(tokenizer_path):\n",
    "        raise FileNotFoundError(f\"Model/Tokenizer không tìm thấy trên worker. Đảm bảo {model_path} và {tokenizer_path} có thể truy cập được.\")\n",
    "        \n",
    "    model = load_model(model_path)\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = load(handle)\n",
    "    \n",
    "    def clean_text_udf(text):\n",
    "        text = str(text).lower()\n",
    "        text = sub(r'[^\\w\\s]', '', text)\n",
    "        text = sub(r'\\d+', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    for comments_batch in iterator:\n",
    "        cleaned_comments = comments_batch.apply(clean_text_udf)\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(cleaned_comments)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        \n",
    "        if len(padded_sequences) > 0:\n",
    "            predictions = model.predict(padded_sequences, verbose=0)\n",
    "            result = [list(map(lambda x: x.tolist(), p)) for p in zip(*predictions)]\n",
    "        else:\n",
    "            result = []\n",
    "\n",
    "        # 5. Trả về batch kết quả\n",
    "        yield Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7159d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "json_fields = [StructField(\"review_text\", StringType())]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    json_fields.append(StructField(aspect, IntegerType(), True)) # True = nullable\n",
    "json_schema = StructType(json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b730ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_SERVER = \"kafka:9092\"\n",
    "TOPIC_NAME = \"review_stream\"\n",
    "\n",
    "kafka_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8aa41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parse JSON và lấy các cột\n",
    "from pyspark.sql.functions import col, from_json\n",
    "parsed_df = kafka_df.select(\n",
    "    col(\"value\").cast(\"string\").alias(\"json_value\")\n",
    ").select(\n",
    "    from_json(col(\"json_value\"), json_schema).alias(\"data\")\n",
    ").select(\"data.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849af474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Áp dụng Pandas UDF để dự đoán (chỉ cần 'review_text')\n",
    "predictions_df = parsed_df.withColumn(\n",
    "    \"predictions_prob\",\n",
    "    predict_sentiments_udf(col(\"review_text\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022dfb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa luồng xử lý (bao gồm nhãn thật).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from numpy import argmax\n",
    "# 4. Giải nén dự đoán và giữ lại nhãn thật\n",
    "# Bắt đầu với dataframe chứa nhãn thật và xác suất dự đoán\n",
    "result_df = predictions_df # predictions_df giờ đã chứa cả cột nhãn thật và predictions_prob\n",
    "\n",
    "# UDF để map ngược index (0, 1, 2, 3) về nhãn (-1, 0, 1, 2)\n",
    "udf_inverser = udf(lambda idx: inverse_label_map.get(idx, -99), IntegerType())\n",
    "\n",
    "for i, aspect in enumerate(ASPECT_COLUMNS):\n",
    "    # UDF để lấy index có xác suất cao nhất\n",
    "    udf_extractor = udf(lambda prob_array: int(argmax(prob_array[i])), IntegerType())\n",
    "\n",
    "    # Lấy ra index dự đoán (0-3)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_idx_{aspect}\",\n",
    "        udf_extractor(col(\"predictions_prob\"))\n",
    "    )\n",
    "    # Ánh xạ ngược index về nhãn gốc (-1, 0, 1, 2)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_{aspect}\",\n",
    "        udf_inverser(col(f\"pred_idx_{aspect}\"))\n",
    "    ).drop(f\"pred_idx_{aspect}\") # Xóa cột index trung gian\n",
    "\n",
    "# Xóa cột xác suất không cần thiết nữa\n",
    "result_df = result_df.drop(\"predictions_prob\")\n",
    "print(\"Đã định nghĩa luồng xử lý (bao gồm nhãn thật).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query đã bắt đầu. Đang chờ dữ liệu từ Kafka...\n",
      "Chạy cell producer 'sendStream_reviews.ipynb' để gửi dữ liệu.\n",
      "Nhấn Interrupt Kernel (nút Stop) để dừng stream.\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = fc9c15a7-58b6-4826-9900-5bf32e44eaa4, runId = 4d8fa1e4-ec1b-416d-af23-4322cc4ecd65] terminated with exception: Failed to create new KafkaAdminClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNhấn Interrupt Kernel (nút Stop) để dừng stream.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐang dừng query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = fc9c15a7-58b6-4826-9900-5bf32e44eaa4, runId = 4d8fa1e4-ec1b-416d-af23-4322cc4ecd65] terminated with exception: Failed to create new KafkaAdminClient"
     ]
    }
   ],
   "source": [
    "# Chọn các cột cuối cùng để hiển thị\n",
    "final_output_df = result_df.select(\n",
    "    \"review_text\",\n",
    "    \"pred_Quality\",\n",
    "    \"pred_Price\",\n",
    "    \"pred_Shipping\",\n",
    "    \"pred_Shop_Service\",\n",
    "    \"pred_Size\",\n",
    "    \"pred_Outlook\",\n",
    "    \"pred_General\",\n",
    "    \"pred_Others\"\n",
    ")\n",
    "\n",
    "# Chạy stream và hiển thị ra console\n",
    "query = final_output_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query đã bắt đầu. Đang chờ dữ liệu từ Kafka...\")\n",
    "print(\"Chạy cell producer 'sendStream_reviews.ipynb' để gửi dữ liệu.\")\n",
    "print(\"Nhấn Interrupt Kernel (nút Stop) để dừng stream.\")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đang dừng query...\")\n",
    "    query.stop()\n",
    "    print(\"Query đã dừng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các cột cuối cùng để ghi vào memory (bao gồm nhãn thật và dự đoán)\n",
    "display_columns = [\"review_text\"]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    display_columns.append(aspect) # Cột nhãn thật\n",
    "    display_columns.append(f\"pred_{aspect}\") # Cột dự đoán\n",
    "\n",
    "final_output_df_mem = result_df.select(*display_columns) # Dùng *\n",
    "\n",
    "# Nếu bạn muốn hiển thị kết quả trong một bảng (table) mà bạn có thể query:\n",
    "query_memory = final_output_df_mem.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"review_predictions_table\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query (memory) đã bắt đầu. Chạy cell tiếp theo để xem kết quả và đánh giá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Đang làm mới... (Nhấn Interrupt Kernel để dừng)\")\n",
    "        # Hiển thị bảng từ memory\n",
    "        display(spark.sql(\"SELECT * FROM review_predictions_table\").toPandas())\n",
    "        time.sleep(5) # Làm mới sau mỗi 5 giây\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đã dừng hiển thị.\")\n",
    "    query_memory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181333d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Bắt đầu Đánh giá Mô hình trên Dữ liệu Đã Thu thập từ Stream ---\")\n",
    "\n",
    "# --- 1. Đọc dữ liệu dự đoán và nhãn thật từ memory sink ---\n",
    "try:\n",
    "    # Bảng này giờ đã chứa cả nhãn thật (vd: 'Price') và dự đoán (vd: 'pred_Price')\n",
    "    eval_df = spark.sql(\"SELECT * FROM review_predictions_table\")\n",
    "\n",
    "    if eval_df.count() == 0:\n",
    "        print(\"Chưa có dữ liệu trong bảng 'review_predictions_table'. Hãy đợi stream chạy.\")\n",
    "    else:\n",
    "        print(f\"Đã đọc {eval_df.count()} bản ghi từ memory sink để đánh giá.\")\n",
    "\n",
    "        # --- 2. Đánh giá từng khía cạnh ---\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "        evaluator_accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "        print(\"\\n--- KẾT QUẢ ĐÁNH GIÁ ---\")\n",
    "        total_accuracy = 0\n",
    "        total_f1 = 0\n",
    "        valid_aspects = 0\n",
    "\n",
    "        for aspect in ASPECT_COLUMNS:\n",
    "            true_col = aspect\n",
    "            pred_col = f\"pred_{aspect}\"\n",
    "\n",
    "            # Chọn cột nhãn và dự đoán, đổi tên, bỏ null\n",
    "            aspect_eval_df = eval_df.select(\n",
    "                col(true_col).cast(\"double\").alias(\"label\"),\n",
    "                col(pred_col).cast(\"double\").alias(\"prediction\")\n",
    "            ).na.drop() # Rất quan trọng: Bỏ qua nếu nhãn thật là null (-99 hoặc None)\n",
    "\n",
    "            count = aspect_eval_df.count()\n",
    "            if count > 0:\n",
    "                f1_score = evaluator_f1.evaluate(aspect_eval_df)\n",
    "                accuracy = evaluator_accuracy.evaluate(aspect_eval_df)\n",
    "                print(f\"Khía cạnh: {aspect} ({count} bản ghi)\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "                total_accuracy += accuracy\n",
    "                total_f1 += f1_score\n",
    "                valid_aspects += 1\n",
    "            else:\n",
    "                 print(f\"Khía cạnh: {aspect} - Không có dữ liệu hợp lệ (non-null) để đánh giá.\")\n",
    "\n",
    "        # Tính trung bình nếu có khía cạnh hợp lệ\n",
    "        if valid_aspects > 0:\n",
    "            avg_accuracy = total_accuracy / valid_aspects\n",
    "            avg_f1 = total_f1 / valid_aspects\n",
    "            print(\"\\n--- Trung bình ---\")\n",
    "            print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"  Average F1-Score: {avg_f1:.4f}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi truy vấn hoặc đánh giá bảng 'review_predictions_table': {e}\")\n",
    "    print(\"Hãy đảm bảo query ghi vào memory sink đang chạy và đã xử lý dữ liệu.\")\n",
    "\n",
    "print(\"\\n--- Đánh giá Hoàn tất ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
