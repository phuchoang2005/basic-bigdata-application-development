{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0185a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_5388/141112755.py\", line 15, in <module>\n",
      "    .getOrCreate()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py\", line 556, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 523, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 207, in __init__\n",
      "    self._do_init(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 249, in _do_init\n",
      "    self._conf = SparkConf(_jvm=SparkContext._jvm)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/conf.py\", line 136, in __init__\n",
      "    self._jconf = jvm.SparkConf(loadDefaults)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1613, in __call__\n",
      "    (new_args, temp_args) = self._get_args(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1598, in _get_args\n",
      "    if converter.can_convert(arg):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py\", line 3311, in can_convert\n",
      "    from pyspark.testing.utils import have_numpy\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/testing/__init__.py\", line 19, in <module>\n",
      "    from pyspark.testing.utils import assertDataFrameEqual, assertSchemaEqual\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/testing/utils.py\", line 97, in <module>\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py\", line 28, in require_minimum_pandas_version\n",
      "    import pandas\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 53, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/conda/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_5388/141112755.py\", line 15, in <module>\n",
      "    .getOrCreate()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py\", line 556, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 523, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 207, in __init__\n",
      "    self._do_init(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/core/context.py\", line 249, in _do_init\n",
      "    self._conf = SparkConf(_jvm=SparkContext._jvm)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/conf.py\", line 136, in __init__\n",
      "    self._jconf = jvm.SparkConf(loadDefaults)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1613, in __call__\n",
      "    (new_args, temp_args) = self._get_args(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1598, in _get_args\n",
      "    if converter.can_convert(arg):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py\", line 3311, in can_convert\n",
      "    from pyspark.testing.utils import have_numpy\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/testing/__init__.py\", line 19, in <module>\n",
      "    from pyspark.testing.utils import assertDataFrameEqual, assertSchemaEqual\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/testing/utils.py\", line 97, in <module>\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py\", line 28, in require_minimum_pandas_version\n",
      "    import pandas\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 67, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/conda/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m spark_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5.1\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Đảm bảo đúng phiên bản Spark của bạn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m packages \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark:spark-sql-kafka-0-10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscala_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,org.apache.kafka:kafka-clients:3.5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCNN and Kafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.ui.showConsoleProgress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m spark\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[1;32m    637\u001b[0m     ):\n\u001b[1;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'  # Đảm bảo đúng phiên bản Spark của bạn\n",
    "\n",
    "packages = f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},org.apache.kafka:kafka-clients:3.5.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CNN and Kafka\") \\\n",
    "    .config(\"spark.jars.packages\", packages) \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151dc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "# Các hằng số từ notebook huấn luyện của bạn\n",
    "MODEL_PATH = 'cnn_multi_aspect_model.h5'\n",
    "TOKENIZER_PATH = 'tokenizer.pickle'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "ASPECT_COLUMNS = ['Price', 'Shipping', 'Outlook', 'Quality', 'Size', 'Shop_Service', 'General', 'Others']\n",
    "\n",
    "# Ánh xạ ngược từ index (0-3) về nhãn gốc (-1, 0, 1, 2)\n",
    "label_map = {-1: 0, 0: 1, 1: 2, 2: 3}\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Broadcast đường dẫn file để các executor có thể thấy\n",
    "sc = spark.sparkContext\n",
    "broadcasted_model_path = sc.broadcast(MODEL_PATH)\n",
    "broadcasted_tokenizer_path = sc.broadcast(TOKENIZER_PATH)\n",
    "\n",
    "# Schema cho đầu ra của UDF: một mảng chứa 8 mảng con (mỗi mảng con 4 xác suất)\n",
    "schema_output = ArrayType(ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_890/2185264633.py\", line 2, in <module>\n",
      "    from pandas import Series\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 53, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/conda/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_890/2185264633.py\", line 2, in <module>\n",
      "    from pandas import Series\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 67, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/conda/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "from pandas import Series\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pyarrow\n",
    "@pandas_udf(schema_output)\n",
    "def predict_sentiments_udf(iterator: Iterator[Series]) -> Iterator[Series]:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from pickle import load\n",
    "    from re import sub\n",
    "    from os import path\n",
    "\n",
    "    model_path = broadcasted_model_path.value\n",
    "    tokenizer_path = broadcasted_tokenizer_path.value\n",
    "    \n",
    "    if not path.exists(model_path) or not path.exists(tokenizer_path):\n",
    "        raise FileNotFoundError(f\"Model/Tokenizer không tìm thấy trên worker. Đảm bảo {model_path} và {tokenizer_path} có thể truy cập được.\")\n",
    "        \n",
    "    model = load_model(model_path)\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = load(handle)\n",
    "    \n",
    "    def clean_text_udf(text):\n",
    "        text = str(text).lower()\n",
    "        text = sub(r'[^\\w\\s]', '', text)\n",
    "        text = sub(r'\\d+', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    for comments_batch in iterator:\n",
    "        cleaned_comments = comments_batch.apply(clean_text_udf)\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(cleaned_comments)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        \n",
    "        if len(padded_sequences) > 0:\n",
    "            predictions = model.predict(padded_sequences, verbose=0)\n",
    "            result = [list(map(lambda x: x.tolist(), p)) for p in zip(*predictions)]\n",
    "        else:\n",
    "            result = []\n",
    "\n",
    "        # 5. Trả về batch kết quả\n",
    "        yield Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7159d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "json_fields = [StructField(\"review_text\", StringType())]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    json_fields.append(StructField(aspect, IntegerType(), True)) # True = nullable\n",
    "json_schema = StructType(json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b730ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_SERVER = \"kafka:9092\"\n",
    "TOPIC_NAME = \"review_stream\"\n",
    "\n",
    "kafka_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8aa41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parse JSON và lấy các cột\n",
    "from pyspark.sql.functions import col, from_json\n",
    "parsed_df = kafka_df.select(\n",
    "    col(\"value\").cast(\"string\").alias(\"json_value\")\n",
    ").select(\n",
    "    from_json(col(\"json_value\"), json_schema).alias(\"data\")\n",
    ").select(\"data.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849af474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Áp dụng Pandas UDF để dự đoán (chỉ cần 'review_text')\n",
    "predictions_df = parsed_df.withColumn(\n",
    "    \"predictions_prob\",\n",
    "    predict_sentiments_udf(col(\"review_text\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022dfb1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m argmax\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 4. Giải nén dự đoán và giữ lại nhãn thật\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Bắt đầu với dataframe chứa nhãn thật và xác suất dự đoán\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_df\u001b[49m \u001b[38;5;66;03m# predictions_df giờ đã chứa cả cột nhãn thật và predictions_prob\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# UDF để map ngược index (0, 1, 2, 3) về nhãn (-1, 0, 1, 2)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m udf_inverser \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;28;01mlambda\u001b[39;00m idx: inverse_label_map\u001b[38;5;241m.\u001b[39mget(idx, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m99\u001b[39m), IntegerType())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from numpy import argmax\n",
    "# 4. Giải nén dự đoán và giữ lại nhãn thật\n",
    "# Bắt đầu với dataframe chứa nhãn thật và xác suất dự đoán\n",
    "result_df = predictions_df # predictions_df giờ đã chứa cả cột nhãn thật và predictions_prob\n",
    "\n",
    "# UDF để map ngược index (0, 1, 2, 3) về nhãn (-1, 0, 1, 2)\n",
    "udf_inverser = udf(lambda idx: inverse_label_map.get(idx, -99), IntegerType())\n",
    "\n",
    "for i, aspect in enumerate(ASPECT_COLUMNS):\n",
    "    # UDF để lấy index có xác suất cao nhất\n",
    "    udf_extractor = udf(lambda prob_array: int(argmax(prob_array[i])), IntegerType())\n",
    "\n",
    "    # Lấy ra index dự đoán (0-3)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_idx_{aspect}\",\n",
    "        udf_extractor(col(\"predictions_prob\"))\n",
    "    )\n",
    "    # Ánh xạ ngược index về nhãn gốc (-1, 0, 1, 2)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_{aspect}\",\n",
    "        udf_inverser(col(f\"pred_idx_{aspect}\"))\n",
    "    ).drop(f\"pred_idx_{aspect}\") # Xóa cột index trung gian\n",
    "\n",
    "# Xóa cột xác suất không cần thiết nữa\n",
    "result_df = result_df.drop(\"predictions_prob\")\n",
    "print(\"Đã định nghĩa luồng xử lý (bao gồm nhãn thật).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query đã bắt đầu. Đang chờ dữ liệu từ Kafka...\n",
      "Chạy cell producer 'sendStream_reviews.ipynb' để gửi dữ liệu.\n",
      "Nhấn Interrupt Kernel (nút Stop) để dừng stream.\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 3a50ebaa-31e5-4894-b8bf-2abafa601d7c, runId = 70a6f806-9002-47cd-8954-991d7e5aec5c] terminated with exception: Failed to create new KafkaAdminClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNhấn Interrupt Kernel (nút Stop) để dừng stream.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐang dừng query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 3a50ebaa-31e5-4894-b8bf-2abafa601d7c, runId = 70a6f806-9002-47cd-8954-991d7e5aec5c] terminated with exception: Failed to create new KafkaAdminClient"
     ]
    }
   ],
   "source": [
    "# Chọn các cột cuối cùng để hiển thị\n",
    "final_output_df = result_df.select(\n",
    "    \"review_text\",\n",
    "    \"pred_Quality\",\n",
    "    \"pred_Price\",\n",
    "    \"pred_Shipping\",\n",
    "    \"pred_Shop_Service\",\n",
    "    \"pred_Size\",\n",
    "    \"pred_Outlook\",\n",
    "    \"pred_General\",\n",
    "    \"pred_Others\"\n",
    ")\n",
    "\n",
    "# Chạy stream và hiển thị ra console\n",
    "query = final_output_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query đã bắt đầu. Đang chờ dữ liệu từ Kafka...\")\n",
    "print(\"Chạy cell producer 'sendStream_reviews.ipynb' để gửi dữ liệu.\")\n",
    "print(\"Nhấn Interrupt Kernel (nút Stop) để dừng stream.\")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đang dừng query...\")\n",
    "    query.stop()\n",
    "    print(\"Query đã dừng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các cột cuối cùng để ghi vào memory (bao gồm nhãn thật và dự đoán)\n",
    "display_columns = [\"review_text\"]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    display_columns.append(aspect) # Cột nhãn thật\n",
    "    display_columns.append(f\"pred_{aspect}\") # Cột dự đoán\n",
    "\n",
    "final_output_df_mem = result_df.select(*display_columns) # Dùng *\n",
    "\n",
    "# Nếu bạn muốn hiển thị kết quả trong một bảng (table) mà bạn có thể query:\n",
    "query_memory = final_output_df_mem.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"review_predictions_table\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query (memory) đã bắt đầu. Chạy cell tiếp theo để xem kết quả và đánh giá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Đang làm mới... (Nhấn Interrupt Kernel để dừng)\")\n",
    "        # Hiển thị bảng từ memory\n",
    "        display(spark.sql(\"SELECT * FROM review_predictions_table\").toPandas())\n",
    "        time.sleep(5) # Làm mới sau mỗi 5 giây\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đã dừng hiển thị.\")\n",
    "    query_memory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181333d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Bắt đầu Đánh giá Mô hình trên Dữ liệu Đã Thu thập từ Stream ---\")\n",
    "\n",
    "# --- 1. Đọc dữ liệu dự đoán và nhãn thật từ memory sink ---\n",
    "try:\n",
    "    # Bảng này giờ đã chứa cả nhãn thật (vd: 'Price') và dự đoán (vd: 'pred_Price')\n",
    "    eval_df = spark.sql(\"SELECT * FROM review_predictions_table\")\n",
    "\n",
    "    if eval_df.count() == 0:\n",
    "        print(\"Chưa có dữ liệu trong bảng 'review_predictions_table'. Hãy đợi stream chạy.\")\n",
    "    else:\n",
    "        print(f\"Đã đọc {eval_df.count()} bản ghi từ memory sink để đánh giá.\")\n",
    "\n",
    "        # --- 2. Đánh giá từng khía cạnh ---\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "        evaluator_accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "        print(\"\\n--- KẾT QUẢ ĐÁNH GIÁ ---\")\n",
    "        total_accuracy = 0\n",
    "        total_f1 = 0\n",
    "        valid_aspects = 0\n",
    "\n",
    "        for aspect in ASPECT_COLUMNS:\n",
    "            true_col = aspect\n",
    "            pred_col = f\"pred_{aspect}\"\n",
    "\n",
    "            # Chọn cột nhãn và dự đoán, đổi tên, bỏ null\n",
    "            aspect_eval_df = eval_df.select(\n",
    "                col(true_col).cast(\"double\").alias(\"label\"),\n",
    "                col(pred_col).cast(\"double\").alias(\"prediction\")\n",
    "            ).na.drop() # Rất quan trọng: Bỏ qua nếu nhãn thật là null (-99 hoặc None)\n",
    "\n",
    "            count = aspect_eval_df.count()\n",
    "            if count > 0:\n",
    "                f1_score = evaluator_f1.evaluate(aspect_eval_df)\n",
    "                accuracy = evaluator_accuracy.evaluate(aspect_eval_df)\n",
    "                print(f\"Khía cạnh: {aspect} ({count} bản ghi)\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "                total_accuracy += accuracy\n",
    "                total_f1 += f1_score\n",
    "                valid_aspects += 1\n",
    "            else:\n",
    "                 print(f\"Khía cạnh: {aspect} - Không có dữ liệu hợp lệ (non-null) để đánh giá.\")\n",
    "\n",
    "        # Tính trung bình nếu có khía cạnh hợp lệ\n",
    "        if valid_aspects > 0:\n",
    "            avg_accuracy = total_accuracy / valid_aspects\n",
    "            avg_f1 = total_f1 / valid_aspects\n",
    "            print(\"\\n--- Trung bình ---\")\n",
    "            print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"  Average F1-Score: {avg_f1:.4f}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi truy vấn hoặc đánh giá bảng 'review_predictions_table': {e}\")\n",
    "    print(\"Hãy đảm bảo query ghi vào memory sink đang chạy và đã xử lý dữ liệu.\")\n",
    "\n",
    "print(\"\\n--- Đánh giá Hoàn tất ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
