{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e772822-f5b5-4692-bbbe-307a85926ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /home/jovyan/.local/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting pandas==2.1.4\n",
      "  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pyarrow==14.0.1\n",
      "  Downloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyspark==3.5.1\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m466.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: findspark==2.0.1 in /opt/conda/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas==2.1.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas==2.1.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas==2.1.4) (2023.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jovyan/.local/lib/python3.11/site-packages (from pyspark==3.5.1) (0.10.9.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.16.0)\n",
      "Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8d61efb5ad24826f0981d8ba0bb36b7cec0a01ea165196084776db5fb9be49d6\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark, pyarrow, pandas\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 21.0.0\n",
      "    Uninstalling pyarrow-21.0.0:\n",
      "      Successfully uninstalled pyarrow-21.0.0\n",
      "Successfully installed pandas-2.1.4 pyarrow-14.0.1 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U numpy==1.26.4 pandas==2.1.4 pyarrow==14.0.1 pyspark==3.5.1 findspark==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5bd463e-031e-45af-a422-e6cbc491b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.13.1 in /home/jovyan/.local/lib/python3.11/site-packages (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (25.9.23)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (1.74.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (3.15.1)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (18.1.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.13.1) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow==2.13.1) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.13.1) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.41.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.9)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jovyan/.local/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jovyan/.local/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jovyan/.local/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/jovyan/.local/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U tensorflow==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fc7c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, pandas_udf, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bd8fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Tắt warning chung của Python\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tắt các thông báo INFO và WARNING của TensorFlow (chỉ hiện ERROR)\n",
    "# Mức 1: Filter INFO. Mức 2: Filter WARNING. Mức 3: Filter ERROR.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "# Tắt logging DEBUG và INFO của TensorFlow (khi sử dụng tf.get_logger)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# --- Thêm dòng này SAU KHI bạn tạo SparkSession ---\n",
    "# Ví dụ: spark = SparkSession.builder...getOrCreate()\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\") # Chỉ hiện lỗi của Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0185a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6550893d05cf:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming_ABSA_CNN</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa9b3f319d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Khởi tạo Spark Session (CHẠY CPU, KHÔNG DÙNG GPU)\n",
    "\n",
    "KAFKA_SERVER = \"kafka:9092\"\n",
    "TOPIC_NAME = \"review_stream\"\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'  # Đảm bảo đúng phiên bản Spark của bạn\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.0'\n",
    "]\n",
    "\n",
    "# Ép tất cả worker và driver không nhìn thấy GPU\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Streaming_ABSA_CNN\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    # bật reuse worker để không load lại model nhiều lần\n",
    "    .config(\"spark.python.worker.reuse\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151dc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hằng số từ notebook huấn luyện của bạn\n",
    "MODEL_PATH = 'cnn_multi_aspect_model.h5'\n",
    "TOKENIZER_PATH = 'tokenizer.pickle'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "ASPECT_COLUMNS = ['Price', 'Shipping', 'Outlook', 'Quality', 'Size', 'Shop_Service', 'General', 'Others']\n",
    "\n",
    "# Ánh xạ ngược từ index (0-3) về nhãn gốc (-1, 0, 1, 2)\n",
    "label_map = {-1: 0, 0: 1, 1: 2, 2: 3}\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Broadcast đường dẫn file để các executor có thể thấy\n",
    "sc = spark.sparkContext\n",
    "broadcasted_model_path = sc.broadcast(MODEL_PATH)\n",
    "broadcasted_tokenizer_path = sc.broadcast(TOKENIZER_PATH)\n",
    "\n",
    "# Schema cho đầu ra của UDF: một mảng chứa 8 mảng con (mỗi mảng con 4 xác suất)\n",
    "schema_output = ArrayType(ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eef7f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_139/4084353176.py\", line 1, in <module>\n",
      "    @pandas_udf(schema_output)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/pandas/functions.py\", line 338, in pandas_udf\n",
      "    require_minimum_pyarrow_version()\n",
      "  File \"/usr/local/spark/python/pyspark/sql/pandas/utils.py\", line 53, in require_minimum_pyarrow_version\n",
      "    import pyarrow\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 4.0.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/utils.py:53\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     have_arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/lib.pyx:36\u001b[0m, in \u001b[0;36minit pyarrow.lib\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@pandas_udf\u001b[39m\u001b[43m(\u001b[49m\u001b[43mschema_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_sentiments_udf\u001b[39m(iterator: Iterator[pd\u001b[38;5;241m.\u001b[39mSeries]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Các import này là BẮT BUỘC bên trong UDF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/functions.py:338\u001b[0m, in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# are not yet visible to the user. Some of behaviors are buggy and might be changed in the near\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# future. The table might have to be eventually documented externally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Note: Timezone is KST.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Note: 'X' means it throws an exception during the conversion.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m--> 338\u001b[0m \u001b[43mrequire_minimum_pyarrow_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# decorator @pandas_udf(returnType, functionType)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m is_decorator \u001b[38;5;241m=\u001b[39m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, (\u001b[38;5;28mstr\u001b[39m, DataType))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/utils.py:60\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_arrow:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pyarrow_version\n\u001b[1;32m     63\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pyarrow\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pyarrow_version):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pyarrow_version, pyarrow\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     68\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: PyArrow >= 4.0.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "@pandas_udf(schema_output)\n",
    "def predict_sentiments_udf(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Các import này là BẮT BUỘC bên trong UDF\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import pickle\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # Tải mô hình và tokenizer MỘT LẦN cho mỗi worker\n",
    "    model_path = broadcasted_model_path.value\n",
    "    tokenizer_path = broadcasted_tokenizer_path.value\n",
    "    \n",
    "    if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):\n",
    "        # Xử lý lỗi nếu file không tồn tại trên worker\n",
    "        raise FileNotFoundError(f\"Model/Tokenizer không tìm thấy trên worker. Đảm bảo {model_path} và {tokenizer_path} có thể truy cập được.\")\n",
    "        \n",
    "    model = load_model(model_path)\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    # Hàm làm sạch văn bản (cần định nghĩa lại bên trong UDF)\n",
    "    def clean_text_udf(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Xóa dấu câu\n",
    "        text = re.sub(r'\\d+', '', text)      # Xóa số\n",
    "        return text.strip()\n",
    "\n",
    "    # Xử lý từng batch (pd.Series) được Spark gửi đến\n",
    "    for comments_batch in iterator:\n",
    "        # 1. Làm sạch văn bản\n",
    "        cleaned_comments = comments_batch.apply(clean_text_udf)\n",
    "        \n",
    "        # 2. Tokenize và Pad\n",
    "        sequences = tokenizer.texts_to_sequences(cleaned_comments)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        \n",
    "        # 3. Dự đoán\n",
    "        if len(padded_sequences) > 0:\n",
    "            # model.predict trả về một list 8 mảng (mỗi mảng cho 1 khía cạnh)\n",
    "            predictions = model.predict(padded_sequences, verbose=0)\n",
    "            \n",
    "            # 4. Định dạng lại đầu ra: [[aspect1_probs], [aspect2_probs], ...]\n",
    "            # Chúng ta cần zip chúng lại để mỗi *hàng* chứa 8 mảng dự đoán\n",
    "            # predictions là [arr_price, arr_shipping, ...]\n",
    "            # zip(*predictions) sẽ nhóm [ (price_row1, ship_row1, ...), (price_row2, ship_row2, ...), ... ]\n",
    "            # Chuyển đổi sang list để serialize\n",
    "            result = [list(map(lambda x: x.tolist(), p)) for p in zip(*predictions)]\n",
    "        else:\n",
    "            result = []\n",
    "\n",
    "        # 5. Trả về batch kết quả\n",
    "        yield pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a918d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema cho tin nhắn JSON từ Kafka (Thêm các cột nhãn)\n",
    "json_fields = [StructField(\"review_text\", StringType())]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    # Nhãn gốc có thể là null nếu dùng None trong producer\n",
    "    json_fields.append(StructField(aspect, IntegerType(), True)) # True = nullable\n",
    "\n",
    "json_schema = StructType(json_fields)\n",
    "\n",
    "\n",
    "# 1. Đọc từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 2. Parse JSON và lấy các cột\n",
    "parsed_df = kafka_df.select(\n",
    "    col(\"value\").cast(\"string\").alias(\"json_value\")\n",
    ").select(\n",
    "    from_json(col(\"json_value\"), json_schema).alias(\"data\")\n",
    "#).select(\"data.review_text\") # Lấy tất cả các cột con từ 'data'\n",
    ").select(\"data.*\") # Sử dụng .* để lấy tất cả các trường trong struct 'data'\n",
    "\n",
    "\n",
    "# 3. Áp dụng Pandas UDF để dự đoán (chỉ cần 'review_text')\n",
    "predictions_df = parsed_df.withColumn(\n",
    "    \"predictions_prob\",\n",
    "    predict_sentiments_udf(col(\"review_text\"))\n",
    ")\n",
    "\n",
    "# 4. Giải nén dự đoán và giữ lại nhãn thật\n",
    "# Bắt đầu với dataframe chứa nhãn thật và xác suất dự đoán\n",
    "result_df = predictions_df # predictions_df giờ đã chứa cả cột nhãn thật và predictions_prob\n",
    "\n",
    "# UDF để map ngược index (0, 1, 2, 3) về nhãn (-1, 0, 1, 2)\n",
    "udf_inverser = udf(lambda idx: inverse_label_map.get(idx, -99), IntegerType())\n",
    "\n",
    "for i, aspect in enumerate(ASPECT_COLUMNS):\n",
    "    # UDF để lấy index có xác suất cao nhất\n",
    "    udf_extractor = udf(lambda prob_array: int(np.argmax(prob_array[i])), IntegerType())\n",
    "\n",
    "    # Lấy ra index dự đoán (0-3)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_idx_{aspect}\",\n",
    "        udf_extractor(col(\"predictions_prob\"))\n",
    "    )\n",
    "    # Ánh xạ ngược index về nhãn gốc (-1, 0, 1, 2)\n",
    "    result_df = result_df.withColumn(\n",
    "        f\"pred_{aspect}\",\n",
    "        udf_inverser(col(f\"pred_idx_{aspect}\"))\n",
    "    ).drop(f\"pred_idx_{aspect}\") # Xóa cột index trung gian\n",
    "\n",
    "# Xóa cột xác suất không cần thiết nữa\n",
    "result_df = result_df.drop(\"predictions_prob\")\n",
    "\n",
    "print(\"Đã định nghĩa luồng xử lý (bao gồm nhãn thật).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0577d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các cột cuối cùng để hiển thị\n",
    "final_output_df = result_df.select(\n",
    "    \"review_text\",\n",
    "    \"pred_Quality\",\n",
    "    \"pred_Price\",\n",
    "    \"pred_Shipping\",\n",
    "    \"pred_Shop_Service\",\n",
    "    \"pred_Size\",\n",
    "    \"pred_Outlook\",\n",
    "    \"pred_General\",\n",
    "    \"pred_Others\"\n",
    ")\n",
    "\n",
    "# Chạy stream và hiển thị ra console\n",
    "query = final_output_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query đã bắt đầu. Đang chờ dữ liệu từ Kafka...\")\n",
    "print(\"Chạy cell producer 'sendStream_reviews.ipynb' để gửi dữ liệu.\")\n",
    "print(\"Nhấn Interrupt Kernel (nút Stop) để dừng stream.\")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đang dừng query...\")\n",
    "    query.stop()\n",
    "    print(\"Query đã dừng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các cột cuối cùng để ghi vào memory (bao gồm nhãn thật và dự đoán)\n",
    "display_columns = [\"review_text\"]\n",
    "for aspect in ASPECT_COLUMNS:\n",
    "    display_columns.append(aspect) # Cột nhãn thật\n",
    "    display_columns.append(f\"pred_{aspect}\") # Cột dự đoán\n",
    "\n",
    "final_output_df_mem = result_df.select(*display_columns) # Dùng *\n",
    "\n",
    "# Nếu bạn muốn hiển thị kết quả trong một bảng (table) mà bạn có thể query:\n",
    "query_memory = final_output_df_mem.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"review_predictions_table\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query (memory) đã bắt đầu. Chạy cell tiếp theo để xem kết quả và đánh giá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Đang làm mới... (Nhấn Interrupt Kernel để dừng)\")\n",
    "        # Hiển thị bảng từ memory\n",
    "        display(spark.sql(\"SELECT * FROM review_predictions_table\").toPandas())\n",
    "        time.sleep(5) # Làm mới sau mỗi 5 giây\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Đã dừng hiển thị.\")\n",
    "    query_memory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181333d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Bắt đầu Đánh giá Mô hình trên Dữ liệu Đã Thu thập từ Stream ---\")\n",
    "\n",
    "# --- 1. Đọc dữ liệu dự đoán và nhãn thật từ memory sink ---\n",
    "try:\n",
    "    # Bảng này giờ đã chứa cả nhãn thật (vd: 'Price') và dự đoán (vd: 'pred_Price')\n",
    "    eval_df = spark.sql(\"SELECT * FROM review_predictions_table\")\n",
    "\n",
    "    if eval_df.count() == 0:\n",
    "        print(\"Chưa có dữ liệu trong bảng 'review_predictions_table'. Hãy đợi stream chạy.\")\n",
    "    else:\n",
    "        print(f\"Đã đọc {eval_df.count()} bản ghi từ memory sink để đánh giá.\")\n",
    "\n",
    "        # --- 2. Đánh giá từng khía cạnh ---\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "        evaluator_accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "        print(\"\\n--- KẾT QUẢ ĐÁNH GIÁ ---\")\n",
    "        total_accuracy = 0\n",
    "        total_f1 = 0\n",
    "        valid_aspects = 0\n",
    "\n",
    "        for aspect in ASPECT_COLUMNS:\n",
    "            true_col = aspect\n",
    "            pred_col = f\"pred_{aspect}\"\n",
    "\n",
    "            # Chọn cột nhãn và dự đoán, đổi tên, bỏ null\n",
    "            aspect_eval_df = eval_df.select(\n",
    "                col(true_col).cast(\"double\").alias(\"label\"),\n",
    "                col(pred_col).cast(\"double\").alias(\"prediction\")\n",
    "            ).na.drop() # Rất quan trọng: Bỏ qua nếu nhãn thật là null (-99 hoặc None)\n",
    "\n",
    "            count = aspect_eval_df.count()\n",
    "            if count > 0:\n",
    "                f1_score = evaluator_f1.evaluate(aspect_eval_df)\n",
    "                accuracy = evaluator_accuracy.evaluate(aspect_eval_df)\n",
    "                print(f\"Khía cạnh: {aspect} ({count} bản ghi)\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "                total_accuracy += accuracy\n",
    "                total_f1 += f1_score\n",
    "                valid_aspects += 1\n",
    "            else:\n",
    "                 print(f\"Khía cạnh: {aspect} - Không có dữ liệu hợp lệ (non-null) để đánh giá.\")\n",
    "\n",
    "        # Tính trung bình nếu có khía cạnh hợp lệ\n",
    "        if valid_aspects > 0:\n",
    "            avg_accuracy = total_accuracy / valid_aspects\n",
    "            avg_f1 = total_f1 / valid_aspects\n",
    "            print(\"\\n--- Trung bình ---\")\n",
    "            print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"  Average F1-Score: {avg_f1:.4f}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi truy vấn hoặc đánh giá bảng 'review_predictions_table': {e}\")\n",
    "    print(\"Hãy đảm bảo query ghi vào memory sink đang chạy và đã xử lý dữ liệu.\")\n",
    "\n",
    "print(\"\\n--- Đánh giá Hoàn tất ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
